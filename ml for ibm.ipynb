{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Toronto Neighborhoods #\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# question 1 #"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## only first table needed, drop others ##"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "import itertools\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from matplotlib.ticker import NullFormatter\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.ticker as ticker\r\n",
    "from sklearn import preprocessing\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "download dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "!wget -O loan_train.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_train.csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "get dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('loan_train.csv')\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "convert datetime object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['due_date'] = pd.to_datetime(df['due_date'])\r\n",
    "df['effective_date'] = pd.to_datetime(df['effective_date'])\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data visualization and pre-processing #\r\n",
    "Letâ€™s see how many of each class is in our data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "df['loan_status'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "260 people have paid off the loan on time while 86 have gone into collection\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets plot some columns to underestand data better:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!conda install -c anaconda seaborn -y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "bins = np.linspace(df.Principal.min(), df.Principal.max(), 10)\r\n",
    "g = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\r\n",
    "g.map(plt.hist, 'Principal', bins=bins, ec=\"k\")\r\n",
    "\r\n",
    "g.axes[-1].legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bins = np.linspace(df.age.min(), df.age.max(), 10)\r\n",
    "g = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\r\n",
    "g.map(plt.hist, 'age', bins=bins, ec=\"k\")\r\n",
    "\r\n",
    "g.axes[-1].legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "# Pre-processing: Feature selection/extraction #\r\n",
    "Lets look at the day of the week people get the loan"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['dayofweek'] = df['effective_date'].dt.dayofweek\r\n",
    "bins = np.linspace(df.dayofweek.min(), df.dayofweek.max(), 10)\r\n",
    "g = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\r\n",
    "g.map(plt.hist, 'dayofweek', bins=bins, ec=\"k\")\r\n",
    "g.axes[-1].legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "# Categorical features to numerical values #\r\n",
    "Lets look at gender:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "86 % of female pay there loans while only 73 % of males pay there loan\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets convert male to 0 and female to 1:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One Hot Encoding #\r\n",
    "How about education?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.groupby(['education'])['loan_status'].value_counts(normalize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature befor One Hot Encoding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df[['Principal','terms','age','Gender','education']].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use one hot encoding technique to conver categorical varables to binary variables and append them to the feature Data Frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Feature = df[['Principal','terms','age','Gender','weekend']]\r\n",
    "Feature = pd.concat([Feature,pd.get_dummies(df['education'])], axis=1)\r\n",
    "Feature.drop(['Master or Above'], axis = 1,inplace=True)\r\n",
    "Feature.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature selection #\r\n",
    "Lets defind feature sets, X:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = Feature\r\n",
    "X[0:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = df['loan_status'].values\r\n",
    "y[0:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "# Normalize Data #\r\n",
    "Data Standardization give data zero mean and unit variance (technically should be done after train test split )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X= preprocessing.StandardScaler().fit(X).transform(X)\r\n",
    "X[0:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split #"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\r\n",
    "print ('Train set:', x_train.shape,  y_train.shape)\r\n",
    "print ('Test set:', x_test.shape,  y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "# Checking for the best value of K #"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "for k in range(1, 10):\r\n",
    "    knn_model  = KNeighborsClassifier(n_neighbors = k).fit(x_train, y_train)\r\n",
    "    knn_yhat = knn_model.predict(x_test)\r\n",
    "    print(\"For K = {} accuracy = {}\".format(k,accuracy_score(y_test,knn_yhat)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"We can see that the KNN model is the best for K=7\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_knn_model = KNeighborsClassifier(n_neighbors = 7).fit(x_train, y_train)\r\n",
    "best_knn_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "## Evaluation Metrics\r\n",
    "# jaccard score and f1 score\r\n",
    "from sklearn.metrics import jaccard_similarity_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "print(\"Train set Accuracy (Jaccard): \", jaccard_similarity_score(y_train, best_knn_model.predict(x_train)))\r\n",
    "print(\"Test set Accuracy (Jaccard): \", jaccard_similarity_score(y_test, best_knn_model.predict(x_test)))\r\n",
    "\r\n",
    "print(\"Train set Accuracy (F1): \", f1_score(y_train, best_knn_model.predict(x_train), average='weighted'))\r\n",
    "print(\"Test set Accuracy (F1): \", f1_score(y_test, best_knn_model.predict(x_test), average='weighted'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# decision trees #"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# importing libraries\r\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for d in range(1,10):\r\n",
    "    dt = DecisionTreeClassifier(criterion = 'entropy', max_depth = d).fit(x_train, y_train)\r\n",
    "    dt_yhat = dt.predict(x_test)\r\n",
    "    print(\"For depth = {}  the accuracy score is {} \".format(d, accuracy_score(y_test, dt_yhat)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "print(\"The best value of depth is d = 2 \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_dt_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 2).fit(x_train, y_train)\r\n",
    "best_dt_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Evaluation Metrics\r\n",
    "# jaccard score and f1 score\r\n",
    "from sklearn.metrics import jaccard_similarity_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "print(\"Train set Accuracy (Jaccard): \", jaccard_similarity_score(y_train, best_dt_model.predict(x_train)))\r\n",
    "print(\"Test set Accuracy (Jaccard): \", jaccard_similarity_score(y_test, best_dt_model.predict(x_test)))\r\n",
    "\r\n",
    "print(\"Train set Accuracy (F1): \", f1_score(y_train, best_dt_model.predict(x_train), average='weighted'))\r\n",
    "print(\"Test set Accuracy (F1): \", f1_score(y_test, best_dt_model.predict(x_test), average='weighted'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Trees #"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#importing svm\r\n",
    "from sklearn import svm \r\n",
    "from sklearn.metrics import f1_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for k in ('linear', 'poly', 'rbf','sigmoid'):\r\n",
    "    svm_model = svm.SVC( kernel = k).fit(x_train,y_train)\r\n",
    "    svm_yhat = svm_model.predict(x_test)\r\n",
    "    print(\"For kernel: {}, the f1 score is: {}\".format(k,f1_score(y_test,svm_yhat, average='weighted')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"We can see the rbf has the best f1 score \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## building best SVM with kernel = rbf\r\n",
    "best_svm = svm.SVC(kernel='rbf').fit(x_train,y_train)\r\n",
    "best_svm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "## Evaluation Metrics\r\n",
    "# jaccard score and f1 score\r\n",
    "from sklearn.metrics import jaccard_similarity_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "print(\"Train set Accuracy (Jaccard): \", jaccard_similarity_score(y_train, best_svm.predict(x_train)))\r\n",
    "print(\"Test set Accuracy (Jaccard): \", jaccard_similarity_score(y_test, best_svm.predict(x_test)))\r\n",
    "\r\n",
    "print(\"Train set Accuracy (F1): \", f1_score(y_train, best_svm.predict(x_train), average='weighted'))\r\n",
    "print(\"Test set Accuracy (F1): \", f1_score(y_test, best_svm.predict(x_test), average='weighted'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression #"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importing libraries\r\n",
    "from sklearn.linear_model import LogisticRegression \r\n",
    "from sklearn.metrics import log_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for k in ('lbfgs', 'saga', 'liblinear', 'newton-cg', 'sag'):\r\n",
    "    lr_model = LogisticRegression(C = 0.01, solver = k).fit(x_train, y_train)\r\n",
    "    lr_yhat = lr_model.predict(x_test)\r\n",
    "    y_prob = lr_model.predict_proba(x_test)\r\n",
    "    print('When Solver is {}, logloss is : {}'.format(k, log_loss(y_test, y_prob)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"We can see that the best solver is liblinear\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Best logistic regression model with liblinear solver\r\n",
    "\r\n",
    "best_lr_model = LogisticRegression(C = 0.01, solver = 'liblinear').fit(x_train, y_train)\r\n",
    "best_lr_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "## Evaluation Metrics\r\n",
    "# jaccard score and f1 score\r\n",
    "from sklearn.metrics import jaccard_similarity_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "print(\"Train set Accuracy (Jaccard): \", jaccard_similarity_score(y_train, best_lr_model.predict(x_train)))\r\n",
    "print(\"Test set Accuracy (Jaccard): \", jaccard_similarity_score(y_test, best_lr_model.predict(x_test)))\r\n",
    "\r\n",
    "print(\"Train set Accuracy (F1): \", f1_score(y_train, best_lr_model.predict(x_train), average='weighted'))\r\n",
    "print(\"Test set Accuracy (F1): \", f1_score(y_test, best_lr_model.predict(x_test), average='weighted'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model evaluation using test set #"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "from sklearn.metrics import jaccard_similarity_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from sklearn.metrics import log_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "download test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "load testset for evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "test_df = pd.read_csv('loan_test.csv')\r\n",
    "test_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# data processing\r\n",
    "test_df['due_date'] = pd.to_datetime(test_df['due_date'])\r\n",
    "test_df['effective_date'] = pd.to_datetime(test_df['effective_date'])\r\n",
    "test_df['dayofweek'] = test_df['effective_date'].dt.dayofweek\r\n",
    "\r\n",
    "test_df['weekend'] = test_df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\r\n",
    "test_df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\r\n",
    "\r\n",
    "Feature1 = test_df[['Principal','terms','age','Gender','weekend']]\r\n",
    "Feature1 = pd.concat([Feature1,pd.get_dummies(test_df['education'])], axis=1)\r\n",
    "Feature1.drop(['Master or Above'], axis = 1,inplace=True)\r\n",
    "\r\n",
    "\r\n",
    "x_loan_test = Feature1\r\n",
    "x_loan_test = preprocessing.StandardScaler().fit(x_loan_test).transform(x_loan_test)\r\n",
    "\r\n",
    "y_loan_test = test_df['loan_status'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Jaccard\r\n",
    "\r\n",
    "# KNN\r\n",
    "knn_yhat = best_knn_model.predict(x_loan_test)\r\n",
    "jacc1 = round(jaccard_similarity_score(y_loan_test, knn_yhat), 2)\r\n",
    "\r\n",
    "# Decision Tree\r\n",
    "dt_yhat = best_dt_model.predict(x_loan_test)\r\n",
    "jacc2 = round(jaccard_similarity_score(y_loan_test, dt_yhat), 2)\r\n",
    "\r\n",
    "# Support Vector Machine\r\n",
    "svm_yhat = best_svm.predict(x_loan_test)\r\n",
    "jacc3 = round(jaccard_similarity_score(y_loan_test, svm_yhat), 2)\r\n",
    "\r\n",
    "# Logistic Regression\r\n",
    "lr_yhat = best_lr_model.predict(x_loan_test)\r\n",
    "jacc4 = round(jaccard_similarity_score(y_loan_test, lr_yhat), 2)\r\n",
    "\r\n",
    "jss = [jacc1, jacc2, jacc3, jacc4]\r\n",
    "jss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# F1_score\r\n",
    "\r\n",
    "# KNN\r\n",
    "knn_yhat = best_knn_model.predict(x_loan_test)\r\n",
    "f1 = round(f1_score(y_loan_test, knn_yhat, average = 'weighted'), 2)\r\n",
    "\r\n",
    "# Decision Tree\r\n",
    "dt_yhat = best_dt_model.predict(x_loan_test)\r\n",
    "f2 = round(f1_score(y_loan_test, dt_yhat, average = 'weighted'), 2)\r\n",
    "\r\n",
    "# Support Vector Machine\r\n",
    "svm_yhat = best_svm.predict(x_loan_test)\r\n",
    "f3 = round(f1_score(y_loan_test, svm_yhat, average = 'weighted'), 2)\r\n",
    "\r\n",
    "# Logistic Regression\r\n",
    "lr_yhat = best_lr_model.predict(x_loan_test)\r\n",
    "f4 = round(f1_score(y_loan_test, lr_yhat, average = 'weighted'), 2)\r\n",
    "\r\n",
    "f1_list = [f1, f2, f3, f4]\r\n",
    "f1_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# log loss\r\n",
    "\r\n",
    "# Logistic Regression\r\n",
    "lr_prob = best_lr_model.predict_proba(x_loan_test)\r\n",
    "ll_list = ['NA','NA','NA', round(log_loss(y_loan_test, lr_prob), 2)]\r\n",
    "ll_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns = ['KNN', 'Decision Tree', 'SVM', 'Logistic Regression']\r\n",
    "index = ['Jaccard', 'F1-score', 'Logloss']\r\n",
    "\r\n",
    "accuracy_df = pd.DataFrame([jss, f1_list, ll_list], index = index, columns = columns)\r\n",
    "accuracy_df1 = accuracy_df.transpose()\r\n",
    "accuracy_df1.columns.name = 'Algorithm'\r\n",
    "accuracy_df1"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "1eaf350aab7daa282de70178293afa19320de413c40451f3db89ccf829116695"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}